{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import os\n",
    "from tqdm import trange\n",
    "import multiprocessing\n",
    "import string, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('quora_duplicate_questions.tsv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [l.split('\\t')[3:] for l in lines[1:]]\n",
    "l = len(lines)\n",
    "\n",
    "def clean(sent):\n",
    "    sent = str(sent).lower()\n",
    "    sent = ''.join('#' if i.isdigit() else i for i in sent if i not in string.punctuation)\n",
    "    sent = re.sub('#+', '#', sent).split()\n",
    "    return sent\n",
    "data = []\n",
    "for i in range(l):\n",
    "    try:\n",
    "        data.append([clean(lines[i][0]), clean(lines[i][1]), lines[i][2]])\n",
    "    except Exception as e:\n",
    "        continue\n",
    "data = [[i[0], i[1], i[2]] for i in data if not (not i[0] or not i[1])]\n",
    "l = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_qs = np.concatenate((np.array(data)[:,0], np.array(data)[:,1]), axis=0)\n",
    "vocab = Counter(chain(*all_qs))\n",
    "vocab = {k:v for k, v in vocab.items() if v >= 2}\n",
    "for i in range(l):\n",
    "    data[i][0] = [x if x in vocab else 'unk' for x in data[i][0]]\n",
    "    data[i][1] = [x if x in vocab else 'unk' for x in data[i][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Word2Vec(np.concatenate([np.array(data)[:, 0], np.array(data)[:, 1]], 0), batch_words=1000, sg=1, negative=64, \n",
    "             min_count=2, iter=2, size=200, workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m.save('embeddings.model')\n",
    "m = Word2Vec.load('embeddings.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "237\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(i) for i in all_qs])\n",
    "print(np.min([len(i) for i in all_qs]))\n",
    "print(max_length)\n",
    "batch_size = 5\n",
    "dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i2w = dict(enumerate(list(vocab.keys()) + ['unk']))\n",
    "w2i = {v:k for k, v in i2w.items()}\n",
    "vocab_size = len(i2w)\n",
    "for i in range(l):\n",
    "    data[i][0] = [w2i[j] for j in data[i][0]] + [vocab_size] * (max_length -len(data[i][0]))\n",
    "    data[i][1] = [w2i[j] for j in data[i][1]] + [vocab_size] * (max_length -len(data[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(a, b, c, ratio = 0.8):\n",
    "    mask = np.random.rand(l) < ratio\n",
    "    return a[mask], b[mask], c[mask], a[~mask], b[~mask], c[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404340, 237) (404340, 237) (404340,)\n"
     ]
    }
   ],
   "source": [
    "q1 = np.array([i[0] for i in data])\n",
    "q2 = np.array([i[1] for i in data])\n",
    "y = np.array([i[2] for i in data])\n",
    "print(q1.shape, q2.shape, y.shape)\n",
    "splitted_data = train_test_split(q1, q2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = np.zeros([vocab_size + 1, dim])\n",
    "for i, j in i2w.items():\n",
    "    vec[i] = m[j]\n",
    "vec[vocab_size] = [0] * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(x, y, z, size):\n",
    "    idx = np.random.choice(range(len(x)), size=size, replace=False)\n",
    "    return x[idx], y[idx], z[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rm -r data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3500/3500 [05:06<00:00, 11.42it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "for i in trange(3500):\n",
    "    a, b, c = batch(q1, q2, y, 15)\n",
    "    np.savez('data/'+str(i)+'.npz', ques1=a, ques2=b, label=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('data/embed.npz', embed=vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
