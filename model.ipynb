{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Duplicate Question Pair Detection using Deep Learning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from multiprocessing import cpu_count\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from tensorflow.contrib.metrics import streaming_accuracy as accuracy\n",
    "from tqdm import trange, tnrange\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "def rmse_loss(outputs, targets):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.sub(targets, outputs))))\n",
    "def activate(outputs, weight_shape, bias_shape, activation=None):\n",
    "    weights = tf.get_variable(\n",
    "        \"weights\", shape=weight_shape, initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0))\n",
    "    biases = tf.get_variable(\"biases\", shape=bias_shape,\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "    affine = tf.matmul(outputs, weights) + biases\n",
    "    if not activation:\n",
    "        return affine\n",
    "    else:\n",
    "        return activation(affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vec = np.load('data/embed.npz')['embed']\n",
    "batch_size = 64\n",
    "max_length = 272\n",
    "dim = 300\n",
    "reg_lambda = 1e-6\n",
    "is_train=True\n",
    "resume_from_checkpoint=False\n",
    "save = True\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default() as graph:\n",
    "    embeds = tf.constant(vec, dtype=tf.float32)\n",
    "    input1 = tf.placeholder(shape=[batch_size, max_length], dtype=tf.int64)\n",
    "    input2 = tf.placeholder(shape=[batch_size, max_length], dtype=tf.int64)\n",
    "    target = tf.placeholder(shape=[batch_size], dtype=tf.float32)\n",
    "    input1_embed = tf.nn.embedding_lookup(embeds, input1)\n",
    "    input2_embed = tf.nn.embedding_lookup(embeds, input2)    \n",
    "    \n",
    "    def sentence_op(inputs):\n",
    "        with tf.variable_scope('gru1'):\n",
    "            cell1 = tf.contrib.rnn.GRUCell(dim)\n",
    "            o, _ = tf.nn.dynamic_rnn(cell=cell1, inputs=input1_embed,\n",
    "                        sequence_length=length(inputs), dtype=tf.float32)\n",
    "        with tf.variable_scope('gru2'):\n",
    "            cell2 = tf.contrib.rnn.GRUCell(dim)\n",
    "            o, s = tf.nn.dynamic_rnn(cell=cell2, inputs=o,\n",
    "                        sequence_length=length(inputs), dtype=tf.float32)     \n",
    "        return s\n",
    "    \n",
    "    with tf.variable_scope('similarity') as scope:\n",
    "        s1 = sentence_op(input1_embed)\n",
    "        scope.reuse_variables()\n",
    "        s2 = sentence_op(input2_embed)\n",
    "        d = tf.concat([tf.abs(tf.subtract(s1, s2)), tf.multiply(s1, s2)], 1)\n",
    "    \n",
    "    with tf.variable_scope('dense1'):\n",
    "        h1 = tf.squeeze(activate(d, [dim * 2, dim], [dim], activation=None))\n",
    "    with tf.variable_scope('dense2'):\n",
    "        h2 = tf.squeeze(activate(h1, [dim, 100], [100], activation=None))\n",
    "    with tf.variable_scope('dense3'):\n",
    "        preds = tf.squeeze(activate(h2, [100, 1], [1], activation=None))\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=preds, labels=target)) +\\\n",
    "        tf.reduce_sum([reg_lambda * tf.nn.l2_loss(x) for x in tf.trainable_variables()])\n",
    "    optimize_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        if resume_from_checkpoint:\n",
    "            saver = tf.train.import_meta_graph('saves/model.ckpt.meta')\n",
    "            saver.restore(sess, tf.train.latest_checkpoint('./saves'))\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "\n",
    "        if not is_train:\n",
    "            p, t = [], []\n",
    "            for i in tnrange(0, 10000, batch_size):\n",
    "                batch_data = np.load('data_test/'+str(i)+'.npz')\n",
    "                a, b, c = batch_data['ques1'], batch_data['ques2'], batch_data['label']\n",
    "                if a.shape[0] != batch_size:\n",
    "                    continue\n",
    "                p.extend(sess.run(preds,  {input1:a, input2:b, target:c}))\n",
    "                t.extend(c)\n",
    "            p = (np.array(p) >= 0.5).astype(np.int32)\n",
    "            t = np.array(t).astype(np.int32)\n",
    "            print(accuracy_score(p, t))\n",
    "        else: \n",
    "            p, t = [], []\n",
    "            avg_loss = 0\n",
    "            tr = tnrange(6000)\n",
    "            for i in tr:\n",
    "                batch_data = np.load('data/'+str(i)+'.npz')\n",
    "                a, b, c = batch_data['ques1'], batch_data['ques2'], batch_data['label']\n",
    "                _, l, x = sess.run([optimize_op, loss, preds], {input1: a, input2:b, target: c})\n",
    "                avg_loss += l\n",
    "                p.extend(x)\n",
    "                t.extend(c)\n",
    "                if i % 100 == 0 and i != 0:\n",
    "                    if save:\n",
    "                        saver.save(sess, 'saves/model.ckpt')\n",
    "                    p = (np.array(p) >= 0.5).astype(np.int32)\n",
    "                    t = np.array(t).astype(np.int32)\n",
    "                    tr.set_description('Loss = {0}, Training batch accuracy = {1}'.format(avg_loss/100, accuracy_score(p, t)))\n",
    "                    p, t = [], []\n",
    "                    avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
